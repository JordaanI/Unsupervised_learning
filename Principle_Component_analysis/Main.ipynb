{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "An attepmt to reduce dimentionality in data by effectively forming a change of basis. The reduction of features can decrease training time in some algorithms.\n",
    "It can be computationally expensive and might lose some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "CI = iris.copy()                    #copy the dataset, its good practise\n",
    "CI.head()                           #Inspect the data, identify the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate the data and labels into two objects\n",
    "data = CI.drop('species', axis=1)\n",
    "labels = CI['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#splitting the data and normalizing it.\n",
    "def split_and_scale(data,labels,size):\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=size, random_state=0)\n",
    "    #test train split with 0.8 test and 0.2 train\n",
    "\n",
    "    #PCA works best on normalized data hence.\n",
    "    scaler = StandardScaler()\n",
    "    data_train = scaler.fit_transform(data_train) #Here scaler is fitted to the data and the dataset is transformed\n",
    "    data_test = scaler.transform(data_test) #Scaled using the previous fit so to be consistant.\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_initialization(n_components,train,test):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    train = pca.fit_transform(train)\n",
    "    test = pca.transform(test)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    return train, test, explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72229951, 0.2397406 , 0.03335483, 0.00460506])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#running pca with no components does not transform the data and only performs the analysis.\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = split_and_scale(data,labels,0.2)\n",
    "\n",
    "data_train, data_test, explained_variance = PCA_initialization(None,data_train,data_test)\n",
    "explained_variance\n",
    "\n",
    "#here we see that the first feature contributes 0.74 of the variance ect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1 components the principle variance is [0.72229951]\n",
      "For 2 components the principle variance is [0.72229951 0.2397406 ]\n",
      "For 3 components the principle variance is [0.72229951 0.2397406  0.03335483]\n",
      "For 4 components the principle variance is [0.72229951 0.2397406  0.03335483 0.00460506]\n"
     ]
    }
   ],
   "source": [
    "data_train_collection = []\n",
    "data_test_collection = []\n",
    "labels_train_collection = []\n",
    "labels_test_collection = []\n",
    "EV_collection = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    data_train, data_test, labels_train, labels_test = split_and_scale(data,labels,0.2)\n",
    "    data_train, data_test, explained_variance = PCA_initialization(i,data_train,data_test)\n",
    "    data_train_collection.append(data_train.copy())\n",
    "    labels_test_collection.append(labels_test.copy())\n",
    "    labels_train_collection.append(labels_train.copy())\n",
    "    data_test_collection.append(data_test.copy())\n",
    "    EV_collection.append(explained_variance.copy())\n",
    "    print(f'For {i} components the principle variance is {explained_variance}')\n",
    "\n",
    "#note the reduction of features with each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example with classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def classifier_accuracy(data_train, data_test, labels_train, labels_test):\n",
    "    classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    classifier.fit(data_train, labels_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    labels_pred = classifier.predict(data_test)\n",
    "    print(f'Accuracy {accuracy_score(labels_test, labels_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9333333333333333\n",
      "Accuracy 0.8\n",
      "Accuracy 0.8\n",
      "Accuracy 0.9\n"
     ]
    }
   ],
   "source": [
    "for data_train, data_test, labels_train, labels_test in zip(\n",
    "                                                            data_train_collection,\n",
    "                                                            data_test_collection,\n",
    "                                                            labels_train_collection,\n",
    "                                                            labels_test_collection):\n",
    "    classifier_accuracy(data_train, data_test, labels_train, labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17eab0df9e3307548a5c6f41d73e01b4dc6a359441bcee24f0d97b016c3af62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
